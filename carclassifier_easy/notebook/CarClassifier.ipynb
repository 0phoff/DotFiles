{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOFcXQTpGyrY"
      },
      "source": [
        "![aiedge](https://ai-edge.be/assets/img/ai-edge.fe085446.png)\n",
        "\n",
        "# CAR CLASSIFIER\n",
        "This notebook will guide you through the process of testing a pretrained MobileNet car/no-car cassification network and exporting it from TensorFlow to TFLite Micro. In addition to that, we also quantize the model to 8-bits.\n",
        "Afterwards, we can run this model on a micro-controller, such as the ESP32.\n",
        "\n",
        "For more information, check out the TensorFlow documentation: https://www.tensorflow.org/lite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFE3-GHvGyrd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "!rm -rf sample_data\n",
        "\n",
        "if not os.path.exists('dataset'):\n",
        "  print('Fetching Data...')\n",
        "  !wget -q -O embeddedai-data.tar.xz https://iiw.kuleuven.be/onderzoek/eavise/embedded-ai-workshop-data/at_download/file\n",
        "  !tar -xf embeddedai-data.tar.xz\n",
        "  print('Data downloaded!\\n')\n",
        "else:\n",
        "  print('Data already downloaded!\\n')\n",
        "\n",
        "print('Installing dependencies...')\n",
        "!apt-get -qq install xxd > /dev/null\n",
        "!pip install tensorflow==2.4.1 > /dev/null\n",
        "print('Dependencies installed!\\n')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "print('Tensorflow Version', tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ5vtiMoGyrf"
      },
      "source": [
        "## Loading the Model\n",
        "\n",
        "Here we will load a model which includes architecture and weight data.  \n",
        "This model has been pretrained on the coco dataset and is stored in the **model/** directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "QzJBB5fVGyrg"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('./model', compile=True)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWuOx9WIGyrg"
      },
      "source": [
        "## Testing the Model\n",
        "Before we quantize and convert the model to TFLite, we first want to run the original tensorflow model through a test dataset.  \n",
        "These results can then be compared with the results from our quantized model on the ESP-EYE, in order to validate our model is working correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZemRjY4Gyrh"
      },
      "source": [
        "### Testing Dataset\n",
        "We supplied a testing dataset in the **dataset/** folder.  \n",
        "The folder contains two subfolders **car/** and **no_car/**, which contain images with and without cars respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8BZQZHIGyrh"
      },
      "outputs": [],
      "source": [
        "test_dataset = image_dataset_from_directory(\n",
        "    './dataset',\n",
        "    class_names=('no_car', 'car'),\n",
        "    shuffle=True,\n",
        "    batch_size=1,\n",
        "    image_size=model.input_shape[1:3],\n",
        "    interpolation='bilinear',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsmGqVGpGyrj"
      },
      "source": [
        "### Testing Accuracy\n",
        "We can now use this dataset to evaluate our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3USr6PUBGyrj"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "print(f'Test Accuracy = {100 * accuracy:.2f}%')\n",
        "print(f'Test loss = {loss:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A3nkpwQGyrk"
      },
      "source": [
        "### Visualize Test Examples\n",
        "We can also use this dataset to visualize some results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "-Ax3hRrTGyrl"
      },
      "outputs": [],
      "source": [
        "# Create a matplotlib plot\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Loop through 9 images\n",
        "for i, (images, labels) in enumerate(test_dataset.take(9)):\n",
        "    \n",
        "    # Create a 3x3 grid of images and select image I\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    \n",
        "    # Plot the image\n",
        "    plt.imshow(images[0].numpy().astype('uint8'))\n",
        "    \n",
        "    # Perform a prediction\n",
        "    predicted_probability = model.predict(images[0:1])[0]\n",
        "    \n",
        "    # Interpret the probability into the correct class 'car' or 'no_car'\n",
        "    predicted_class = ### TODO ###\n",
        "    \n",
        "    # Add ground truth and prediction as a title\n",
        "    plt.title(\n",
        "        f'Ground Truth = {test_dataset.class_names[labels[0]]}\\n'\n",
        "        f'Prediction = {predicted_class}'\n",
        "    )\n",
        "    \n",
        "    # Disable plotting axis\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyNO13_PGyrl"
      },
      "source": [
        "## Exporting the Model\n",
        "Now we need to export the model to TFLite.  \n",
        "This time, we will also use TFLite to quantize the model to int8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XE7R6QqGyrm"
      },
      "source": [
        "### Workaround a bug\n",
        "\n",
        "Since Tensorflow 2.3 there is [a bug](https://github.com/tensorflow/tensorflow/issues/45256) with the reshape layer when this layer is converted to tensorflow lite.  \n",
        "For some reason the converter produces a non supported sequence of `Shape → Strided Slice → Pack → Reshape` for each reshape layer instead of just using the shape layer.\n",
        "Unfortunately, earlier Tensorflow versions that do not have this bug, have limited INT8 support which we want to use.\n",
        "\n",
        "We work around this bug by removing the reshape layer in our tensorflow model.  \n",
        "The only disadvantage is that the output will have a shape of `(None, 1, 1, 1)` instead of `(None, 1)`, so we need to take this into consideration.\n",
        "\n",
        "Note that this fix removes the preprocessing layers too which would be removed by the converter anyway if kept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3x3XF8BGyrn"
      },
      "outputs": [],
      "source": [
        "# ORIGINAL MODEL\n",
        "if len(model.layers) == 4:\n",
        "  # Select only the model part\n",
        "  model_new = model.layers[-1]\n",
        "\n",
        "  # Select the output of the layer just before the reshape layer\n",
        "  x = model_new.layers[-3].output\n",
        "\n",
        "  # Add an activation function on top\n",
        "  x = tf.keras.layers.Activation(activation='sigmoid', name='predictions')(x)\n",
        "\n",
        "  # Compose the new model\n",
        "  model_new = tf.keras.Model(model_new.input, x)\n",
        "\n",
        "# PRUNED MODEL\n",
        "elif len(model.layers) == 91:\n",
        "  # Create new sequential from the model layers until the reshape layer\n",
        "  model_new = tf.keras.Sequential()\n",
        "  for layer in model.layers[5:-2]:\n",
        "    model_new.add(layer)\n",
        "\n",
        "  # Add an activation function on top\n",
        "  model_new.add(tf.keras.layers.Activation(activation='sigmoid', name='predictions'))\n",
        "\n",
        "  # Build new model\n",
        "  model_new.build((None, 96, 96, 3))\n",
        "  \n",
        "else:\n",
        "  raise NotImplementedError('Unkown model variant')\n",
        "\n",
        "\n",
        "# Print model summary\n",
        "model = model_new\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPBQR7XSGyrn"
      },
      "source": [
        "### Quantized TFLite Model\n",
        "In addition to the the standard conversion, we are also going tell the Tensorflow Lite converter to apply Full integer quantization.  \n",
        "_You can use the [following guide](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only) to fill in the blanks in the next code section._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI4HFoXcGyro"
      },
      "outputs": [],
      "source": [
        "# Create a representative dataset for quantization\n",
        "def representative_dataset():\n",
        "    for images, _ in test_dataset.take(100):\n",
        "        for image in images:\n",
        "            res = np.expand_dims(image.numpy() / 255., axis=0).astype(np.float32)\n",
        "            yield [res]\n",
        "\n",
        "# Create the TFLite converter from our Keras model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Enable full integer quantization\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.optimizations = ### TODO ###\n",
        "converter.target_spec.supported_ops = ### TODO ###\n",
        "converter.inference_input_type = ### TODO ###\n",
        "converter.inference_output_type = ### TODO ###\n",
        "\n",
        "# Convert our model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save tflite model to a file\n",
        "with open('carclassifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtBv6xs0Gyro"
      },
      "source": [
        "We have now saved our TFLite model to a file, which we can visualize with Netron.  \n",
        "\n",
        "First, download the **carclassifier.tflite** file from google colab.  \n",
        "Click on the folder icon in the left sidebar and the right-click the file and download it to your computer.\n",
        "\n",
        "Then, <a href=\"https://netron.app\" target=\"_blank\">Click here</a> to open netron and select your file to visualize it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ipEtRTwGyrp"
      },
      "source": [
        "### Convert to C++ data\n",
        "Finally, we need to convert the TFLite model to a C++ array.\n",
        "\n",
        "> \n",
        "> _Do not forget to download **carclassifier.cpp** to your computer, after running the cell below._\n",
        "> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKwkgAa5Gyrp"
      },
      "outputs": [],
      "source": [
        "!xxd -i carclassifier.tflite > carclassifier.cpp\n",
        "!cat carclassifier.cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFnUZy5kGyrp"
      },
      "source": [
        "## Compare Model Probabilities\n",
        "As a basic sanity check, we will compare the output probabilities of our TensorFlow model with the quantized TFLite model.  \n",
        "If all went well, the difference should be pretty small.\n",
        "\n",
        "The advantage of comparing the outputs as opposed to looking at the testing accuracy, is that this code could theoretically be used for any network, regardless of its task.\n",
        "We indeed simply look at the output tensors and compute the MSE between our floating point and quantized model.\n",
        "\n",
        "_You can use the [following guide](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python) to fill in the blanks in the next code section (scroll down to the second code block)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OUzzEo7Gyrq"
      },
      "outputs": [],
      "source": [
        "# Load the TFLite model from the path './carclassifier.tflite'\n",
        "interpreter = tf.lite.Interpreter('./carclassifier.tflite')\n",
        "\n",
        "# Allocate memory\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input & output details\n",
        "# These are necessary to know where we need to place the input in memory and where we get the output\n",
        "input_details = ### TODO ###\n",
        "output_details = ### TODO ###\n",
        "\n",
        "# Arrays to store the output probabilities\n",
        "tf_outputs = []\n",
        "tflite_outputs = []\n",
        "\n",
        "# Loop through 25 images\n",
        "for image, label in test_dataset.take(25):\n",
        "    \n",
        "    # Convert the input image ([0,255] uint8) for our quantized model ([0,1] float32)\n",
        "    tf_input = tf.cast(image / 255, tf.float32)\n",
        "    \n",
        "    # Predict using tensorflow model\n",
        "    tf_output = model.predict(tf_input)\n",
        "    \n",
        "    # Append probability to list\n",
        "    tf_outputs.append(tf_output)\n",
        "    \n",
        "    \n",
        "    # Convert the input image ([0,255] uint8) for our quantized model ([-128,127] int8)\n",
        "    # HINT: Use tf.cast()\n",
        "    tflite_input = ### TODO ###\n",
        "\n",
        "    # Predict using TFLite model\n",
        "    interpreter.set_tensor(### TODO ###)\n",
        "    interpreter.invoke()\n",
        "    tflite_output = interpreter.get_tensor(### TODO ###)\n",
        "\n",
        "    # Convert the model output ([-128,127] int8) to a probability ([0,1] float32)\n",
        "    tflite_output = ### TODO ###\n",
        "\n",
        "    # Append probability to list\n",
        "    tflite_outputs.append(tflite_output)\n",
        "\n",
        "    \n",
        "# Compute the Mean Squared Error\n",
        "tf_outputs = np.concatenate(tf_outputs)\n",
        "tflite_outputs = np.concatenate(tflite_outputs)\n",
        "mse = ((tf_outputs - tflite_outputs) ** 2).mean()\n",
        "\n",
        "print(tf_outputs.shape, tflite_outputs.shape)\n",
        "print(\"Mean squared error =\", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkhLI6THGyrr"
      },
      "source": [
        "![eavise](https://gitlab.com/EAVISE/branding/logo/-/raw/master/twitter/header_500.png)"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}